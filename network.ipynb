{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pandas.read_csv('publications.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse topics\n",
    "import ast\n",
    "frames = []\n",
    "for index, row in data.iterrows():\n",
    "    frame = {}\n",
    "    s = ast.literal_eval(row['all_topics'])\n",
    "    subs = s[1:]\n",
    "    subs.sort()\n",
    "    frame['id'] = row['article_id']\n",
    "    frame['main'] = row['main_topic'].split('.')[0]\n",
    "    frame['subs'] = subs\n",
    "    frame['simstr'] = ','.join(list(subs))\n",
    "    frames.append(frame)\n",
    "print(frames)\n",
    "\n",
    "df = pandas.DataFrame(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity(a,b):\n",
    "    n = a.split(',')\n",
    "    m = b.split(',')\n",
    "    return SequenceMatcher(None, n, m).ratio()\n",
    "\n",
    "\n",
    "# Calculate edge weights\n",
    "def generate_edge_weights(df: pandas.DataFrame):\n",
    "    edges = {}\n",
    "    for thing in df.itertuples():\n",
    "        if len(thing.simstr) == 0:\n",
    "            continue\n",
    "        \n",
    "        df2 = df[df['main'] != thing.main]\n",
    "        df2['weight'] = df['simstr'].apply(lambda x: calculate_similarity(thing.simstr, x))\n",
    "        links = df2[df2['weight'] >= 0.5]\n",
    "        linked_topics = list(links['main'].value_counts().index[links['main'].value_counts() >= 2])\n",
    "\n",
    "\n",
    "        for topic in linked_topics:\n",
    "            pair = (thing.main, topic)\n",
    "            if pair in edges.keys():\n",
    "                edges[pair] +=1\n",
    "            elif pair[::-1] in edges.keys():\n",
    "                pair = pair[::-1]\n",
    "                edges[pair] +=1\n",
    "            else:\n",
    "                edges[pair] = 1\n",
    "    return edges\n",
    "\n",
    "edges = generate_edge_weights(df)\n",
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adjancency matrix\n",
    "x = set()\n",
    "for pair in edges.keys():\n",
    "    a,b = pair\n",
    "    x.add(a)\n",
    "    x.add(b)\n",
    "\n",
    "x = list(x)\n",
    "side = len(x)\n",
    "matrix = np.zeros((side, side))\n",
    "print(x)\n",
    "for i, row in enumerate(matrix):\n",
    "    for j, col in enumerate(row):\n",
    "        pair = (x[i], x[j])\n",
    "        if pair in edges.keys():\n",
    "            matrix[i][j] = edges[pair]\n",
    "        if pair[::-1] in edges.keys():\n",
    "            matrix[i][j] = edges[pair[::-1]]\n",
    "adj_matrix = pandas.DataFrame(matrix, x, x)\n",
    "\n",
    "print(adj_matrix)\n",
    "adj_matrix.to_excel('adj_matrix.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Normalize weights\n",
    "max_val = max(edges.values())\n",
    "min_val = min(edges.values())\n",
    "\n",
    "for pair, weight in edges.items():\n",
    "    edges[pair] = (weight - min_val) / max_val\n",
    "\n",
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tuples\n",
    "tuples = []\n",
    "nodes = set()\n",
    "for pair, weight in edges.items():\n",
    "    a, b = pair\n",
    "    nodes.add(a)\n",
    "    nodes.add(b)\n",
    "    tuple = (a, b, weight)\n",
    "    tuples.append(tuple)\n",
    "# Create graph\n",
    "\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(list(nodes))\n",
    "graph.add_weighted_edges_from(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print key characteristics\n",
    "__length = len(graph.nodes)\n",
    "__connected = [x for x in nx.connected.connected_components(graph)]\n",
    "__cent = nx.betweenness_centrality(graph)\n",
    "_max_cent = max(__cent.values())\n",
    "_inf_node = [x for x in __cent if __cent[x] == _max_cent]\n",
    "print(graph)\n",
    "print('Degree centrality {:.2f}'.format(sum(nx.degree_centrality(graph).values()) / __length))\n",
    "print('Closeness {:.2f}'.format(sum(nx.closeness_centrality(graph).values()) / __length))\n",
    "print('Betweenness {:.2f}'.format(sum(nx.betweenness_centrality(graph).values()) / __length))\n",
    "print('Clustering coefficient {:.2f}'.format( sum(nx.clustering(graph).values()) / __length))\n",
    "print('Connected components', len(__connected))\n",
    "print('Diameter', nx.distance_measures.diameter(graph))\n",
    "print('Average shortest path length {:.2f}'.format(nx.average_shortest_path_length(graph)))\n",
    "print('Most influential node', _inf_node[0])\n",
    "# diameter, pathlength, clustering coefficient, connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the result\n",
    "nx.draw(graph, pos=nx.kamada_kawai_layout(graph), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate communities\n",
    "import itertools\n",
    "comp = nx.community.girvan_newman(graph)\n",
    "limited = itertools.takewhile(lambda x: len(x) <= 100, comp)\n",
    "communities = [ list(x) for x in next(comp)]\n",
    "singles = []\n",
    "multis = []\n",
    "for com in communities:\n",
    "    if len(com) == 1:\n",
    "        singles.append(com[0])\n",
    "    else:\n",
    "        multis.append(com)\n",
    "print('Singles', len(singles), ', Multis', len(multis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'green', 'blue', 'orange', 'yellow', 'magenta', 'cyan']\n",
    "color_map = []\n",
    "for node in graph:\n",
    "    if node in singles:\n",
    "        color_map.append('grey')\n",
    "    else:\n",
    "        for i, m in enumerate(multis):\n",
    "            if node in m:\n",
    "                color_map.append(colors[i])\n",
    "nx.draw(graph, node_color=color_map, pos=nx.kamada_kawai_layout(graph), with_labels=True)\n",
    "plt.title('Communities by color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph, node_color=color_map, pos=nx.spring_layout(graph), with_labels=True)\n",
    "plt.title('Communities by color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition quality\n",
    "print(\"Graph modularity {:.2f}\".format(nx.community.modularity(graph, communities)))\n",
    "print('Graph partition quality: Coverage {0:.2f}, Performance {1:.2f}'.format(*nx.community.partition_quality(graph, communities)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
